{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17086b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jai/anaconda3/envs/deeplearning/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 394\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"../data/lyrics_dataset.json\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3a0814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Eminem', 1: 'Future', 2: 'Hozier', 3: 'The Weeknd'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(list(set(example[\"label\"] for example in dataset)))\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ced523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 394/394 [00:05<00:00, 78.75 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "dataset = dataset.map(lambda e: {\"label\": label2id[e[\"label\"]]}, remove_columns=[\"label\"])\n",
    "dataset = dataset.map(tokenize_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b0dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='297' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/297 01:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.412200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.828200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.895700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.720900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.527900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.748600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.819100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.619500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.341500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.383600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=297, training_loss=0.7472092804282603, metrics={'train_runtime': 106.0862, 'train_samples_per_second': 11.142, 'train_steps_per_second': 2.8, 'total_flos': 311002852073472.0, 'train_loss': 0.7472092804282603, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/roberta-base\", num_labels=len(label2id))\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results/roberta-base\",\n",
    "    # evaluation_strategy=\"no\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14fc45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_weeknd = \"Write a new song in the style of The Weeknd about heartbreak at night while he was high on ecstasy and oxycontin. He mentions about prioritising his career over his love which led to the heartbreak. YOur output should only consist of the lyrics and nothing else. NO [verse1] or [chorus] or even pre chorus or instruments or anything like that. Only and only the lyrics, no instructions whatsoever either.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3913cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_lyrics_future = \"\"\"\n",
    "I'm drifting under neon lights,  \n",
    "With echoes of your touch last night,  \n",
    "My heart's a beat behind the sound,  \n",
    "But you're not here, I'm breaking down.\"\"\"\n",
    "\n",
    "prompt_future = f\"\"\"Continue this song in the same style and emotion:\\n\\n{seed_lyrics_future}\\nYOur output should only consist of the lyrics and nothing else. NO [verse1] or [chorus] or even pre chorus or instruments or anything like that. Only and only the lyrics, no instructions whatsoever either.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89b46a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_lyrics_hoz = \"\"\"\n",
    "I found your voice beneath cathedral rain,  \n",
    "Like hymns carved deep in sacred stone,  \n",
    "Your touch still lingers in the flame,  \n",
    "A ghost that sings when I’m alone.\"\"\"\n",
    "\n",
    "prompt_hoz = f\"\"\"You're a poetic lyricist like Hozier, known for soulful and haunting metaphors, blending love and spirituality. Continue this song in the same tone and rhythm:\\n\\n{seed_lyrics_hoz}\\nYOur output should only consist of the lyrics and nothing else. NO [verse1] or [chorus] or even pre chorus or instruments or anything like that. Only and only the lyrics, no instructions whatsoever either.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0595ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_lyrics_eminem = \"\"\"\n",
    "Back when I was thirteen, had rage in my veins,  \n",
    "Mom workin' doubles just to handle the pain,  \n",
    "No heat in the house, just beats in my brain,  \n",
    "Used rap as my shelter, escaped in the rain.\"\"\"\n",
    "\n",
    "prompt_eminem = f\"\"\"You're a rapper like Eminem, telling raw, emotional stories with tight rhyme schemes and fast flow. Continue these lyrics with intensity and rhythm:\\n\\n{seed_lyrics_eminem}\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2e9dba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Beat drops - heavy 808s, frantic hi-hats, a distorted piano chord)\n",
      "\n",
      "Yeah... \n",
      "\n",
      "Back when I was thirteen, had rage in my veins, \n",
      "Mom workin' doubles just to handle the pain, \n",
      "No heat in the house, just beats in my brain, \n",
      "Used rap as my shelter, escaped in the rain. \n",
      "\n",
      "Concrete jungle echoes, whispers of shame, \n",
      "Another eviction notice, another losing game. \n",
      "Dad a ghost, a shadow, a forgotten name, \n",
      "Just empty promises etched in the sodium flame. \n",
      "\n",
      "Words like weapons, spitting fire and frost, \n",
      "Turnin' frustration into a lyrical holocaust. \n",
      "Each rhyme a shard of anger, a desperate cost, \n",
      "Tryna build a fortress, before my future’s lost. \n",
      "\n",
      "Used to stare at the streetlight, a sodium glow, \n",
      "Feelin' smaller than the darkness, nowhere left to go. \n",
      "Then the beat hit different, a pulse, a furious flow, \n",
      "Turnin' self-doubt to venom, watchin' the demons grow. \n",
      "\n",
      "(Tempo increases - hi-hats become more rapid, snare hits more pronounced)\n",
      "\n",
      "See, the silence was a killer, a suffocating shroud, \n",
      "Drowning in the shame, lost in the restless crowd. \n",
      "So I traded the sorrow for a lyrical shroud, \n",
      "Turnin' broken pieces into something loud. \n",
      "\n",
      "Yeah, this ain't a brag, this is a battle cry,\n",
      "For every kid trapped beneath a shattered sky. \n",
      "This is the sound of survival, raw and unforgiven, \n",
      "A testament to the rage that keeps us livin'. \n",
      "\n",
      "(Beat intensifies, fades slightly to build tension)\n",
      "\n",
      "...And believe me, the rain still falls. \n",
      "It just sounds a little different now. \n",
      "It’s the rhythm of my scars, you know? \n",
      "(Beat cuts abruptly, leaving only a subtle, distorted echo) \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def generate_lyrics(prompt):\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", \"gemma3:4b\"],\n",
    "        input=prompt.encode(),\n",
    "        capture_output=True\n",
    "    )\n",
    "    return result.stdout.decode()\n",
    "\n",
    "weeknd_gemma = generate_lyrics(prompt_eminem)\n",
    "print(weeknd_gemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bc5fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"results/checkpoint-297\")  # or the saved model dir\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"results/roberta-base/checkpoint-297\")  # or the saved model dir\n",
    "model.eval()\n",
    "\n",
    "# Your id2label dictionary from before\n",
    "id2label = {0: 'Eminem', 1: 'Future', 2: 'Hozier', 3: 'The Weeknd'}  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fad87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_artist(lyrics_text):\n",
    "    inputs = tokenizer(lyrics_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_class_id = torch.argmax(outputs.logits).item()\n",
    "        return id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86a9673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Predicted Artist Style: Eminem\n"
     ]
    }
   ],
   "source": [
    "predicted_artist = predict_artist(weeknd_gemma)\n",
    "print(f\"🧠 Predicted Artist Style: {predicted_artist}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
